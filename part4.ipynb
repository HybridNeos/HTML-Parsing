{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Previously in part 3, we tried multiple models and settled on the Support Vector Machine (SVM) as the best model to go forward with. In addition to performing the best of all models selected in part 3, SVM has many parameters to tune which lends itself well to exploration. In the previous part, our models were unable to beat the single feature baseline. I suspect that with more advanced hyperparameter tuning and with more shuffles of the data we will be able to notably improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load the data\n",
    "From part 3, we tried many variations of the training dataset. Here, I have settled on the min-max scaled dataset because it performed better in most scenarios from part 3. Moreover, SVM is sensitive to data scaling and using the scaled dataset did improve training time upon initial testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"./TFRRS-API/data/processed/raw.csv\")\n",
    "df.drop([\"school\", \"q1\", \"q3\", \"grade\"], axis=1, inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"event_num\"] = le.fit_transform(df[\"event\"])\n",
    "\n",
    "X_scaled = pd.read_csv(\"./TFRRS-API/data/processed/scaled.csv\")\n",
    "y_all_am = df[\"all_american\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train Test split\n",
    "Recall at our data consists of 18 events which were split 13/5 to form the train test data. Because we are using the random library and fixing the seed we obtain the same train/test split as used in part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "test_numbers = random.choices(range(18), k=5)\n",
    "\n",
    "test_indices = df[df[\"event_num\"].isin(test_numbers)].index\n",
    "X_test = X_scaled.iloc[test_indices]\n",
    "y_test = y_all_am.iloc[test_indices]\n",
    "\n",
    "train_indices = df[~df[\"event_num\"].isin(test_numbers)].index\n",
    "X_train = X_scaled.iloc[train_indices]\n",
    "y_train = y_all_am.iloc[train_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Custom metric function\n",
    "\n",
    "Recall that this model is aiming to solve a ranking problem while treating all event groups equally. Using an arbitrary example, it is possible for the 10th best 1500m runner to get a higher predicted probability than the 1st place Javelin thrower. This potential group imbalance between leads to sklearn's built-in metrics not being the most effective. Thus, I have implemented my custom metric function from part 3 as a scoring function for cross validation.\n",
    "\n",
    "This function will take predictions and, based on the training data index, map the values to our original data. From there, places gets the eight highest predicted athletes within each event group and counts what percentage of them were actually all-americans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Function from part 3\n",
    "def percent_all_american_score(y_true, y_pred, indices):  # =df.index):\n",
    "    truth = df.iloc[indices][[\"name\", \"event\", \"place\", \"all_american\"]].copy()\n",
    "    truth[\"prob_all_am\"] = y_pred\n",
    "    top_eight = (\n",
    "        truth.sort_values([\"event\", \"prob_all_am\"], ascending=False)\n",
    "        .groupby([\"event\"])\n",
    "        .head(8)\n",
    "    )\n",
    "    return np.mean(top_eight[\"all_american\"])\n",
    "\n",
    "\n",
    "# Make it compatible with sklearn and allow arbitrary X datasets\n",
    "def test_score_func(train_x, y_pred):\n",
    "    return percent_all_american_score(None, y_pred, indices=train_x.index)\n",
    "\n",
    "\n",
    "my_scorer = make_scorer(test_score_func, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Initial Hyperparameter Selection\n",
    "When evaluating SVM as our model, we must consider the options of kernels to select. As we saw in part 3, the sigmoid kernel performed much notably worse than other options so I am leaving it out. Due to the performance considerations of our cross validation involving many splits, I will be doing an initial grid search between linear, polynomial, and rbf to determine the best kernel to optimize further. Moreover, I will use a simple range of C and the two non-float gamma arguments, for the appropriate kernels, to test the kernels under some variation.\n",
    "\n",
    "With the hyperparameter strategy decided, recall that the dataset has 18 groups which are split into 13/5 for train/test. I will test the hyperparameters by further making a 10/3 split for train/validation and averaging model performance over the possible permutation of the train/validation groups. We need to average over many splits of data because we only have one year's worth of data so the model can be sensitive to which events it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, LeavePGroupsOut\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "C_vals = [10**x for x in range(-2, 2)]\n",
    "gamma_vals = [\"auto\", \"scale\"]\n",
    "\n",
    "params = [\n",
    "    # Linear\n",
    "    {\"kernel\": [\"linear\"], \"C\": C_vals},\n",
    "    # Poly\n",
    "    {\n",
    "        \"kernel\": [\"poly\"],\n",
    "        \"degree\": range(2, 5),\n",
    "        \"C\": C_vals,\n",
    "        \"coef0\": [0, 0.5, 1],\n",
    "        \"gamma\": gamma_vals,\n",
    "    },\n",
    "    # Rbf\n",
    "    {\"kernel\": [\"rbf\"], \"C\": C_vals, \"gamma\": gamma_vals},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Using support vector classifier as our model\n",
    "svm = SVC(probability=True, class_weight=\"balanced\", random_state=0)\n",
    "\n",
    "# This creates the 10/3 split\n",
    "num_groups = 3\n",
    "splitter = LeavePGroupsOut(n_groups=num_groups)\n",
    "\n",
    "# Create the grid search object\n",
    "clf = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=params,\n",
    "    scoring=my_scorer,\n",
    "    n_jobs=4,\n",
    "    cv=splitter,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 286 folds for each of 84 candidates, totalling 24024 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/svm_cross_val.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model and save results\n",
    "clf.fit(X_train, y_train, groups=df.iloc[X_train.index][\"event_num\"])\n",
    "joblib.dump(clf, f\"./models/svm_cross_val.pkl\", compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_degree</th>\n",
       "      <th>param_coef0</th>\n",
       "      <th>param_gamma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.591200</td>\n",
       "      <td>0.117019</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.587558</td>\n",
       "      <td>0.102893</td>\n",
       "      <td>poly</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.586393</td>\n",
       "      <td>0.122187</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.585810</td>\n",
       "      <td>0.114082</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.585664</td>\n",
       "      <td>0.119465</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.580128</td>\n",
       "      <td>0.110523</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.579254</td>\n",
       "      <td>0.108778</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.578817</td>\n",
       "      <td>0.100312</td>\n",
       "      <td>linear</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.576632</td>\n",
       "      <td>0.109372</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.574009</td>\n",
       "      <td>0.118459</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  std_test_score param_kernel param_C param_degree  \\\n",
       "41         0.591200        0.117019         poly       1            2   \n",
       "58         0.587558        0.102893         poly      10            2   \n",
       "46         0.586393        0.122187         poly       1            2   \n",
       "47         0.585810        0.114082         poly       1            2   \n",
       "52         0.585664        0.119465         poly       1            2   \n",
       "53         0.580128        0.110523         poly       1            2   \n",
       "48         0.579254        0.108778         poly       1            3   \n",
       "2          0.578817        0.100312       linear       1          NaN   \n",
       "54         0.576632        0.109372         poly       1            3   \n",
       "37         0.574009        0.118459         poly     0.1            3   \n",
       "\n",
       "   param_coef0 param_gamma  \n",
       "41           0       scale  \n",
       "58           0        auto  \n",
       "46         0.5        auto  \n",
       "47         0.5       scale  \n",
       "52           1        auto  \n",
       "53           1       scale  \n",
       "48         0.5        auto  \n",
       "2          NaN         NaN  \n",
       "54           1        auto  \n",
       "37           1       scale  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\n",
    "    item: val for item, val in clf.cv_results_.items() if not item.startswith(\"split\")\n",
    "}\n",
    "\n",
    "# Turn the results into a dataframe for visualization\n",
    "# Show the 10 best models found during grid search\n",
    "cv_results = pd.DataFrame(results)\n",
    "cv_results.sort_values([\"mean_test_score\", \"std_test_score\"], ascending=[False, True])[\n",
    "    [\n",
    "        \"mean_test_score\",\n",
    "        \"std_test_score\",\n",
    "        \"param_kernel\",\n",
    "        \"param_C\",\n",
    "        \"param_degree\",\n",
    "        \"param_coef0\",\n",
    "        \"param_gamma\",\n",
    "    ]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Initial Analysis\n",
    "In line with what we determined in part 3, the polynomial kernel is clearly the best for this model. Moreover, degree 2 is clearly the best performing degree with it taking six of the top ten spots. This is encouraging news because while higher degree models are more complex thus can fit the data better they can also lead to overfitting. One thing to note that the model performance here is actually worse than what was found in part 3. However, this is not concerning because as mentioned previously the small dataset makes the model dependent on the random shuffle; during training I observed that the model did score over 0.7 on individual validation sets however it's the average performance on unknown data we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Further tuning strategy\n",
    "Now that we know which kernel is best for the data, as well as knowing what degree to use, we will optimize the model further by trying more granular values of C and gamma. Given that the best and second-best model performances were with C=1 and C=10 respectively I will include more values of C in that range. Regarding gamma, I noticed that large values of gamma significantly slow down training when we have so many splits thus they are not feasible for cross validation on my machine. Luckily, if you look ahead you will notice that the best performance comes with smaller values of gamma regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimized = {\n",
    "    \"C\": [0.1, 0.5, 1, 3, 5, 10],\n",
    "    \"coef0\": [0, 0.5, 1],\n",
    "    \"gamma\": [10**x for x in range(-5, 1)] + [\"auto\", \"scale\"],\n",
    "}\n",
    "\n",
    "# Fixed this to second degree polynomial SVM based on first tuning\n",
    "svm2 = SVC(\n",
    "    kernel=\"poly\", degree=2, probability=True, class_weight=\"balanced\", random_state=0\n",
    ")\n",
    "\n",
    "# Create the grid search object\n",
    "clf2 = GridSearchCV(\n",
    "    estimator=svm2,\n",
    "    param_grid=optimized,\n",
    "    scoring=my_scorer,\n",
    "    n_jobs=4,\n",
    "    cv=splitter,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 286 folds for each of 144 candidates, totalling 41184 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/final_svm_cross_val.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model and save results\n",
    "clf2.fit(X_train, y_train, groups=df.iloc[X_train.index][\"event_num\"])\n",
    "joblib.dump(clf2, f\"./models/final_svm_cross_val.pkl\", compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_coef0</th>\n",
       "      <th>param_gamma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.591200</td>\n",
       "      <td>0.117019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.590618</td>\n",
       "      <td>0.116276</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.121284</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.589452</td>\n",
       "      <td>0.105566</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.588578</td>\n",
       "      <td>0.112945</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.588432</td>\n",
       "      <td>0.105072</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.587558</td>\n",
       "      <td>0.102893</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.586976</td>\n",
       "      <td>0.111739</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.586976</td>\n",
       "      <td>0.122716</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.586830</td>\n",
       "      <td>0.118005</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_test_score  std_test_score param_C param_coef0 param_gamma\n",
       "55          0.591200        0.117019       1           0       scale\n",
       "78          0.590618        0.116276       3           0        auto\n",
       "139         0.589744        0.121284      10           1        0.01\n",
       "100         0.589452        0.105566       5           0         0.1\n",
       "76          0.588578        0.112945       3           0         0.1\n",
       "79          0.588432        0.105072       3           0       scale\n",
       "126         0.587558        0.102893      10           0        auto\n",
       "102         0.586976        0.111739       5           0        auto\n",
       "44          0.586976        0.122716     0.5           1         0.1\n",
       "60          0.586830        0.118005       1         0.5         0.1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the grid search results pull out data excluding the individual splits\n",
    "optimized_results = {\n",
    "    item: val for item, val in clf2.cv_results_.items() if not item.startswith(\"split\")\n",
    "}\n",
    "\n",
    "# Turn the results into a dataframe for visualization\n",
    "# Show the 10 best models found during grid search\n",
    "df_opt = pd.DataFrame(optimized_results)\n",
    "df_opt.sort_values([\"mean_test_score\", \"std_test_score\"], ascending=[False, True])[\n",
    "    [\"mean_test_score\", \"std_test_score\", \"param_C\", \"param_coef0\", \"param_gamma\"]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1262240553486386\n"
     ]
    }
   ],
   "source": [
    "print(clf2.best_estimator_._gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Final Analysis\n",
    "Interestingly, the further hyperparameter tuning did not produce a model better than what we already found previously. That being said, we can still make observations incase we want to impose more regularization in hopes of giving a more generalizable model. The first item that stands out to me is how clearly values around 0.1 for gamma perform best with the model. Thus, we don't want to make large changes to gamma otherwise model performance will likely suffer. Regarding C, while my intuition of adding values between 1 and 10 paid off based on nine out of ten best models being in that range, fortunately the best model has C=1 so we don't have decrease regularization for a better fit.\n",
    "\n",
    "Overall, we clearly found some pattern in the data based on our range of hyperparameters with the top 10 models all being within 1% accuracy of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing\n",
    "Now, we will use our best model (which is the same from both grid search objects) to predict the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675\n"
     ]
    }
   ],
   "source": [
    "preds = clf2.predict_proba(X_test)[:, 1]\n",
    "print(percent_all_american_score(None, preds, indices=X_test.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Shockingly, we get a higher test accuracy than validation accuracy. I would attribute this to having so aforementioend relatively few data samples. For further analysis, let's dig into the errors and examine who the model missed as placing all-american"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "                           name event  place  all_american  prob_all_am\n263             CLAYTON FRITSCH    PV      2             1     0.666345\n270               ZACH BRADFORD    PV      9             0     0.516245\n267              ZACH MCWHORTER    PV      6             1     0.468099\n262           SONDRE GUTTORMSEN    PV      1             1     0.445348\n266               BRANSON ELLIS    PV      5             1     0.411566\n272               CALEB WITSKEN    PV     11             0     0.364169\n274                EERIK HAAMER    PV     13             0     0.350894\n271           TREVOR STEPHENSON    PV     10             0     0.336603\n407                 ETHAN DABBS    JT      2             1     0.289275\n406             MARC MINICHELLO    JT      1             1     0.235698\n410  CHINECHEREM PROSPER NNAMDI    JT      5             1     0.217827\n418                   CORD NEAL    JT     13             0     0.210785\n409                TARAN TAYLOR    JT      4             1     0.208526\n408              TZURIEL PEDIGO    JT      3             1     0.207247\n411             ARTHUR PETERSEN    JT      6             1     0.196182\n412                  DJ JONSSON    JT      7             1     0.185221\n73             NAVASKY ANDERSON   800      2             1     0.588581\n74               BRANDON MILLER   800      3             1     0.508389\n75                  JASON GOMEZ   800      4             1     0.500000\n85                     TIM ZEPF   800     14             0     0.500000\n86                 AYMAN ZAHAFI   800     15             0     0.465708\n78                SAMUEL RODMAN   800      7             1     0.451654\n88                COLLIN EBLING   800     17             0     0.449108\n93                 LUIS PERALTA   800     22             0     0.439028\n196        QUIVELL JORDAN-BACOT  400H      5             1     0.624835\n198              COLTEN YARDLEY  400H      7             1     0.551997\n194           ISAIAH LEVINGSTON  400H      3             1     0.488816\n197             DRAKE SCHNEIDER  400H      6             1     0.478662\n208              MOITALEL MPOKE  400H     17             0     0.443333\n192                SEAN BURRELL  400H      1             1     0.441454\n199                 JAMES SMITH  400H      8             1     0.410526\n193              MALIK METIVIER  400H      2             1     0.385858\n44               DEMARIUS SMITH   200     21             0     0.728907\n27               MICAIAH HARRIS   200      4             1     0.560387\n24            JOSEPH FAHNBULLEH   200      1             1     0.521917\n25               MATTHEW BOLING   200      2             1     0.492096\n29            SHAUN MASWANGANYI   200      6             1     0.488339\n32                ERIC HARRISON   200      9             0     0.486067\n31             COURTNEY LINDSEY   200      8             1     0.484318\n47              JAVONTE HARDING   200     24             0     0.479169",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>event</th>\n      <th>place</th>\n      <th>all_american</th>\n      <th>prob_all_am</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>263</th>\n      <td>CLAYTON FRITSCH</td>\n      <td>PV</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.666345</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>ZACH BRADFORD</td>\n      <td>PV</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0.516245</td>\n    </tr>\n    <tr>\n      <th>267</th>\n      <td>ZACH MCWHORTER</td>\n      <td>PV</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.468099</td>\n    </tr>\n    <tr>\n      <th>262</th>\n      <td>SONDRE GUTTORMSEN</td>\n      <td>PV</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.445348</td>\n    </tr>\n    <tr>\n      <th>266</th>\n      <td>BRANSON ELLIS</td>\n      <td>PV</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0.411566</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>CALEB WITSKEN</td>\n      <td>PV</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0.364169</td>\n    </tr>\n    <tr>\n      <th>274</th>\n      <td>EERIK HAAMER</td>\n      <td>PV</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0.350894</td>\n    </tr>\n    <tr>\n      <th>271</th>\n      <td>TREVOR STEPHENSON</td>\n      <td>PV</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0.336603</td>\n    </tr>\n    <tr>\n      <th>407</th>\n      <td>ETHAN DABBS</td>\n      <td>JT</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.289275</td>\n    </tr>\n    <tr>\n      <th>406</th>\n      <td>MARC MINICHELLO</td>\n      <td>JT</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.235698</td>\n    </tr>\n    <tr>\n      <th>410</th>\n      <td>CHINECHEREM PROSPER NNAMDI</td>\n      <td>JT</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0.217827</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>CORD NEAL</td>\n      <td>JT</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0.210785</td>\n    </tr>\n    <tr>\n      <th>409</th>\n      <td>TARAN TAYLOR</td>\n      <td>JT</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.208526</td>\n    </tr>\n    <tr>\n      <th>408</th>\n      <td>TZURIEL PEDIGO</td>\n      <td>JT</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.207247</td>\n    </tr>\n    <tr>\n      <th>411</th>\n      <td>ARTHUR PETERSEN</td>\n      <td>JT</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.196182</td>\n    </tr>\n    <tr>\n      <th>412</th>\n      <td>DJ JONSSON</td>\n      <td>JT</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0.185221</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>NAVASKY ANDERSON</td>\n      <td>800</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.588581</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>BRANDON MILLER</td>\n      <td>800</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.508389</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>JASON GOMEZ</td>\n      <td>800</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>TIM ZEPF</td>\n      <td>800</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>AYMAN ZAHAFI</td>\n      <td>800</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0.465708</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>SAMUEL RODMAN</td>\n      <td>800</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0.451654</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>COLLIN EBLING</td>\n      <td>800</td>\n      <td>17</td>\n      <td>0</td>\n      <td>0.449108</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>LUIS PERALTA</td>\n      <td>800</td>\n      <td>22</td>\n      <td>0</td>\n      <td>0.439028</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>QUIVELL JORDAN-BACOT</td>\n      <td>400H</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0.624835</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>COLTEN YARDLEY</td>\n      <td>400H</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0.551997</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>ISAIAH LEVINGSTON</td>\n      <td>400H</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.488816</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>DRAKE SCHNEIDER</td>\n      <td>400H</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.478662</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>MOITALEL MPOKE</td>\n      <td>400H</td>\n      <td>17</td>\n      <td>0</td>\n      <td>0.443333</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>SEAN BURRELL</td>\n      <td>400H</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.441454</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>JAMES SMITH</td>\n      <td>400H</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0.410526</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>MALIK METIVIER</td>\n      <td>400H</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.385858</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>DEMARIUS SMITH</td>\n      <td>200</td>\n      <td>21</td>\n      <td>0</td>\n      <td>0.728907</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>MICAIAH HARRIS</td>\n      <td>200</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.560387</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>JOSEPH FAHNBULLEH</td>\n      <td>200</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.521917</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>MATTHEW BOLING</td>\n      <td>200</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.492096</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>SHAUN MASWANGANYI</td>\n      <td>200</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.488339</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>ERIC HARRISON</td>\n      <td>200</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0.486067</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>COURTNEY LINDSEY</td>\n      <td>200</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0.484318</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>JAVONTE HARDING</td>\n      <td>200</td>\n      <td>24</td>\n      <td>0</td>\n      <td>0.479169</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "clf = joblib.load(\"models/final_svm_cross_val.pkl\")\n",
    "preds = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "model_test_df = df.iloc[X_test.index].copy()\n",
    "model_test_df[\"prob_all_am\"] = preds\n",
    "\n",
    "# Get the top eight in each event\n",
    "model_top_eight = (\n",
    "    model_test_df.sort_values([\"event\", \"prob_all_am\"], ascending=False)\n",
    "    .groupby([\"event\"])\n",
    "    .head(8)\n",
    ")\n",
    "model_top_eight[[\"name\", \"event\", \"place\", \"all_american\", \"prob_all_am\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>event</th>\n",
       "      <th>place</th>\n",
       "      <th>prob_all_am</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>MOAD ZAHAFI</td>\n",
       "      <td>800</td>\n",
       "      <td>1</td>\n",
       "      <td>0.306351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>UDODI ONWUZURIKE</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.401963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>KEATON DANIEL</td>\n",
       "      <td>PV</td>\n",
       "      <td>3</td>\n",
       "      <td>0.247668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NATHANIEL EZEKIEL</td>\n",
       "      <td>400H</td>\n",
       "      <td>4</td>\n",
       "      <td>0.378536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>SIMEN GUTTORMSEN</td>\n",
       "      <td>PV</td>\n",
       "      <td>4</td>\n",
       "      <td>0.321034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TARSIS OROGOT</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.417426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>SEAN DOLAN</td>\n",
       "      <td>800</td>\n",
       "      <td>5</td>\n",
       "      <td>0.398337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>DAYTON CARLSON</td>\n",
       "      <td>800</td>\n",
       "      <td>6</td>\n",
       "      <td>0.395270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ROBERT GREGORY</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "      <td>0.476354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>CLAYTON SIMMS</td>\n",
       "      <td>PV</td>\n",
       "      <td>7</td>\n",
       "      <td>0.237704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>JONATHAN SCHWIND</td>\n",
       "      <td>800</td>\n",
       "      <td>8</td>\n",
       "      <td>0.430707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>KYLE RADEMEYER</td>\n",
       "      <td>PV</td>\n",
       "      <td>8</td>\n",
       "      <td>0.263592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>AHMED MAGOUR</td>\n",
       "      <td>JT</td>\n",
       "      <td>8</td>\n",
       "      <td>0.118162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name event  place  prob_all_am\n",
       "72         MOAD ZAHAFI   800      1     0.306351\n",
       "26    UDODI ONWUZURIKE   200      3     0.401963\n",
       "264      KEATON DANIEL    PV      3     0.247668\n",
       "195  NATHANIEL EZEKIEL  400H      4     0.378536\n",
       "265   SIMEN GUTTORMSEN    PV      4     0.321034\n",
       "28       TARSIS OROGOT   200      5     0.417426\n",
       "76          SEAN DOLAN   800      5     0.398337\n",
       "77      DAYTON CARLSON   800      6     0.395270\n",
       "30      ROBERT GREGORY   200      7     0.476354\n",
       "268      CLAYTON SIMMS    PV      7     0.237704\n",
       "79    JONATHAN SCHWIND   800      8     0.430707\n",
       "269     KYLE RADEMEYER    PV      8     0.263592\n",
       "413       AHMED MAGOUR    JT      8     0.118162"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append predictions to a copy of the test data\n",
    "model_test_df = df.iloc[X_test.index].copy()\n",
    "model_test_df[\"prob_all_am\"] = preds\n",
    "\n",
    "# Get the top eight in each event\n",
    "model_top_eight = (\n",
    "    model_test_df.sort_values([\"event\", \"prob_all_am\"], ascending=False)\n",
    "    .groupby([\"event\"])\n",
    "    .head(8)\n",
    ")\n",
    "\n",
    "# Find all americans which were not in our predicted top eight\n",
    "missed_all_americans = model_test_df[\n",
    "    (~model_test_df[\"name\"].isin(model_top_eight[\"name\"]))\n",
    "    & (model_test_df[\"all_american\"] == 1)\n",
    "][[\"name\", \"event\", \"place\", \"prob_all_am\"]].sort_values(\"place\")\n",
    "missed_all_americans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The missed predictions are quite uniform in terms of events and placement positions missed. There is no clear pattern in the false negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "The test data performance is better than expected and has gotten closer to the training baseline accuracy of nearly 73% using a single feature. Now, if we use that same baseline on the test data we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_test_df = df.iloc[X_test.index].copy()\n",
    "column_top_eight = (\n",
    "    model_test_df.sort_values([\"event\", \"mean_three_best\"], ascending=False)\n",
    "    .groupby([\"event\"])\n",
    "    .head(8)\n",
    ")\n",
    "np.mean(column_top_eight[\"all_american\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "which follows the trends of doing better on the test data and being better than our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The conclusion to this project is that machine learning was not well suited to solve this problem. The models I trained did outperform random guessing but they were heavily carried by our singular feature baseline. While I am glad that my subject area expertise of track and field allowed me to create such a useful singular feature, I am also surprised that adding information quickly worsens model performance. I suspect that with more years of data, the dataset will be more resilient while also ena new features based on historical championship performance. Regardless, this was a good exercise in machine learning as I was able to explore multiple models and effectively tune hyperparameters to improve the performance of the ill-fated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Checking assumption of group independence\n",
    "While the following does not affect my conclusions on the model, I wanted to do further analysis for my own interest\n",
    "\n",
    "In part 1 of the project, I set forth an assumption that we can model all events equally. Because the smaller event groups only have 54 samples, creating an ensemble model based on event groups would have uncomfortably high variance. However, I want to use the currently trained model to check the assumption that event groups can be modeled equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the event groups and invert the selection for mapping\n",
    "event_groups = {\n",
    "    \"sprints\": [\"100\", \"110H\", \"200\", \"400\"],\n",
    "    \"mid distance\": [\"400H\", \"800\", \"1500\"],\n",
    "    \"distance\": [\"3000S\", \"5000\", \"10000\"],\n",
    "    \"jumps\": [\"HJ\", \"PV\", \"LJ\", \"TJ\"],\n",
    "    \"throws\": [\"SP\", \"DT\", \"HT\", \"JT\"],\n",
    "}\n",
    "inverse_events = {v: k for k, l in event_groups.items() for v in l}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_group</th>\n",
       "      <th>misses_per_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>throws</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distance</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mid distance</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jumps</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sprints</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    event_group  misses_per_event\n",
       "1        throws              2.00\n",
       "0      distance              2.67\n",
       "2  mid distance              3.00\n",
       "3         jumps              3.00\n",
       "4       sprints              3.00"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the entire dataset\n",
    "temp = df[[\"place\", \"all_american\", \"event\", \"name\"]].copy()\n",
    "all_preds = clf2.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# Map the event to its group\n",
    "temp[\"prob_all_am\"] = all_preds\n",
    "temp[\"event_group\"] = df[\"event\"].apply(lambda x: inverse_events[x])\n",
    "\n",
    "# Note where the model missed all americans\n",
    "top_eight = (\n",
    "    temp.sort_values([\"event\", \"prob_all_am\"], ascending=False)\n",
    "    .groupby([\"event\"])\n",
    "    .head(8)\n",
    ")\n",
    "missed = temp[(~temp[\"name\"].isin(top_eight[\"name\"])) & (temp[\"all_american\"] == 1)][\n",
    "    [\"name\", \"event\", \"place\", \"prob_all_am\", \"event_group\"]\n",
    "]\n",
    "\n",
    "# Average missed by number of events in event group\n",
    "average_misses = pd.DataFrame(\n",
    "    missed.groupby(\"event_group\")[\"event\"]\n",
    "    .count()\n",
    "    .sort_values()\n",
    "    .reset_index()\n",
    "    .rename({\"event\": \"misses\"}, axis=1)\n",
    ")\n",
    "average_misses[\"misses_per_event\"] = average_misses.apply(\n",
    "    lambda x: round(x[\"misses\"] / len(event_groups[x[\"event_group\"]]), 2), axis=1\n",
    ")\n",
    "average_misses[[\"event_group\", \"misses_per_event\"]].sort_values(\"misses_per_event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "While these results are not dramatically different I do think they hint at the assumption of model independence breaking down. Throws are the most distinct track and field event and coincidentally had the fewest errors. Given that jumps are also distinct from the running events, why doesn't the model perform better on jumps as well? Regardless more data would be needed as this year could have just had clear favorites in throws while having tightly contested running events."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}